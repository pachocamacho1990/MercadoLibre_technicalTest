{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1602040992323,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "MEFjKr3KFRE1"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "path = 'drive/My Drive/Colab Notebooks/'\n",
    "country_label = 'MCO'\n",
    "\n",
    "def plot_model_performance(model, X_train, X_test, y_train, y_test):\n",
    "    regr = model.fit(X_train, y_train)\n",
    "    print('train score: {}'.format(regr.score(X_train, y_train)))\n",
    "    predictions = regr.predict(X_train)\n",
    "    sns.regplot(x = y_train, y = predictions)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.show()\n",
    "    print('test score: {}'.format(regr.score(X_test, y_test)))\n",
    "    predictions = regr.predict(X_test)\n",
    "    sns.regplot(x = y_test, y = predictions)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.show()\n",
    "\n",
    "def parse_dic(x, field):\n",
    "    try:\n",
    "        return x[field]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# en esta clase se configura el modo debug para no exceder \n",
    "# el valor offset = 1000 ya que no se esta usando un access token de developer\n",
    "# De esta manera solo trabajamos con una muestra por categoria de maximo 1000 datapoints\n",
    "\n",
    "class mercadolibreAPI: \n",
    "\n",
    "    def __init__(self, country_label, debug = True):\n",
    "        self.debug = debug\n",
    "        self.ml_url = 'http://api.mercadolibre.com/sites/{}/search?q='.format(country_label)\n",
    " \n",
    "    def request_get(self, url):\n",
    "        #if (self.debug): print(\"Procesando url: \", url)\n",
    "        try:\n",
    "            return requests.get(url).json()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def transform(self, ):\n",
    "        self.item_df['number_pictures'] =  self.item_df['pictures'].apply(lambda x: len(x))\n",
    "        self.item_df['days'] =  self.item_df['date_created'].apply(\n",
    "            lambda x: datetime.datetime.today()-datetime.datetime.strptime(x.split('T')[0], '%Y-%m-%d')\n",
    "            )\n",
    "        self.item_df['days'] =  self.item_df['days'].apply(lambda x: x.days)\n",
    "\n",
    "    def get_items_info(self,):\n",
    "        self.objects = []\n",
    "        if self.item_list is not None:\n",
    "            for id in self.item_list:\n",
    "                item_url = 'http://api.mercadolibre.com/items/{}'.format(id)\n",
    "                jsdata = self.request_get(item_url)\n",
    "                if (jsdata is not None): \n",
    "                        self.objects.append(jsdata)\n",
    "            self.item_df = pd.DataFrame(self.objects)\n",
    " \n",
    "    def search_items(self, query, ):\n",
    "        self.items = []\n",
    "        url = self.ml_url + query\n",
    "        print(\"Buscando: \" + url)\n",
    "        if (self.debug):\n",
    "            paginators = 20\n",
    "        else:\n",
    "            paginators = round(self.request_get(url)['paging']['total']/50)+1\n",
    "        for offset in range(0,paginators):\n",
    "            url = self.ml_url + query + '&offset=' + str(offset*50)\n",
    "            jsdata = self.request_get(url)\n",
    "            if (jsdata is not None):\n",
    "                try:  \n",
    "                    self.items = self.items + jsdata['results']\n",
    "                except:\n",
    "                    print(jsdata)\n",
    "\n",
    "        self.df = pd.DataFrame(self.items)\n",
    "        self.item_list = ml.df['id'].values\n",
    "        self.get_items_info()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAb97k_7Fe87"
   },
   "source": [
    "# Extracción de los datos e identificación de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "METpPbu3DokH"
   },
   "source": [
    "Aquí se extrae un muestra de datos con la keyword `smart TV`. De esto se obtiene un sample de 1000 datapoints donde el dataframe `ml.item_df` contiene los atributos de las publicaciones como columnas así como la variable objetivo `sold_quantity`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 173498,
     "status": "ok",
     "timestamp": 1602031362658,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "oP6TnyohKn1X",
    "outputId": "68c6d9bc-ca89-46dd-c67c-e72a5def81cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: http://api.mercadolibre.com/sites/MCO/search?q=smart%20Tv\n"
     ]
    }
   ],
   "source": [
    "ml = mercadolibreAPI(country_label)\n",
    "ml.search_items('smart%20Tv')\n",
    "ml.transform()\n",
    "# aqui solo se muestran algunos features en el dataframe, \n",
    "ml.item_df[['id', 'title', 'days', 'price', 'number_pictures', 'sold_quantity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdxUHF_fEzmr"
   },
   "source": [
    "Inicialmente se considera buscar correlaciones con variables más sencillas:. \n",
    "* `days`: antiguedad de la publicación.\n",
    "* `price`: precio del item\n",
    "* `number_pictures`: numero de imagenes en la publicación.\n",
    "\n",
    "Pero la conclusión es que la muestra de datos usada no evidencia candidato viable a predictor para un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "executionInfo": {
     "elapsed": 13631,
     "status": "ok",
     "timestamp": 1602031440674,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "Ip28e-kZYtaZ",
    "outputId": "802bbf64-4ea4-4aca-849d-6e8bf94b213a"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(ml.item_df[['days', 'price', 'number_pictures', 'sold_quantity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylbuXfWrFqzt"
   },
   "source": [
    "# Atributos de las publicaciones como atributos\n",
    "\n",
    "Dado que la tarea era un modelo de regresión con atributos que tipicamente son categoricos, se consideró una transformación a features dummy que resultaron en vectores de features de más de 1000 componentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggsehqq0k1Xb"
   },
   "source": [
    "## Fully sparse predictors with dummy transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7223,
     "status": "ok",
     "timestamp": 1602036544982,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "6PIlO8t51jJf",
    "outputId": "a27f05ba-6359-47b0-9323-605ec52794b8"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "Y = []\n",
    "for element, result in zip(ml.item_df['attributes'].values, ml.item_df['sold_quantity'].values):\n",
    "    if len(element)>0:\n",
    "        Y.append(result)\n",
    "        tt = pd.DataFrame(element)[['id', 'value_name']].set_index('id').T\n",
    "        df = pd.concat([tt, df])\n",
    "\n",
    "X = df[['BRAND', 'COLOR', 'DISPLAY_SIZE', 'DISPLAY_TYPE', \n",
    "    'IS_3D', 'IS_CURVED', 'IS_PORTABLE', 'IS_SMART', 'OPERATIVE_SYSTEM', 'RESOLUTION_TYPE',\n",
    "    'WITH_HDMI', 'WITH_HDR', 'WITH_USB', 'WITH_VGA', 'WITH_WI_FI',\n",
    "    'BRIGHTNESS', 'CONTRAST_RATIO', 'LINE', 'RESPONSE_TIME',\n",
    "    'STORAGE_CAPACITY', 'ACCESSORIES_INCLUDED', \n",
    "    'DEPTH', 'HDMI_PORTS_NUMBER', 'HEIGHT', 'MAX_RESOLUTION',\n",
    "    'MAX_SPEAKERS_POWER', 'SOUND_MODES', 'SPEAKERS_NUMBER',\n",
    "    'USB_PORTS_NUMBER', 'WEIGHT', 'WIDTH', 'WITH_AUTO_POWER_OFF',\n",
    "    'WITH_BLUETOOTH', 'WITH_WEB_BROWSER', 'PROCESSOR_CORES_NUMBER',\n",
    "    'WITH_ETHERNET', 'WITH_INTEGRATED_VOICE_COMMAND', 'WITH_NETFLIX',\n",
    "    'WITH_SCREEN_SHARE_FUNCTION', 'WITH_YOUTUBE', 'DISPLAY_TECHNOLOGY']]\n",
    "X = pd.get_dummies(data = X, drop_first=True).values\n",
    "y = ml.item_df['sold_quantity'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=44, test_size = 0.1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC6VuJJpF10W"
   },
   "source": [
    "Se utilizan dos modelos a saber de Scikit: \n",
    "\n",
    "* `RandomForestRegressor`\n",
    "* `MLPRegressor`\n",
    "\n",
    "Los cuales logran hacer un match con la data de entrenamiento (mejor en el caso del MLP) pero en ambos casos con un fuerte overfit (ver graficos con la data de test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "executionInfo": {
     "elapsed": 13068,
     "status": "ok",
     "timestamp": 1602036552205,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "59nqZEI3y59M",
    "outputId": "a9b4318f-bd4a-403e-c8e8-9c438c25c421"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_depth=40, random_state=0)\n",
    "plot_model_performance(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "executionInfo": {
     "elapsed": 70302,
     "status": "ok",
     "timestamp": 1602036613275,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "xomfgpgYxwNd",
    "outputId": "b5fd6005-12e2-499f-be5e-9fad2bb5f33c"
   },
   "outputs": [],
   "source": [
    "model = MLPRegressor(random_state=1, max_iter=10000)\n",
    "plot_model_performance(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI3GIK6pk8Sv"
   },
   "source": [
    "## PCA predictors \n",
    "\n",
    "En ciertos casos la codificacion por variables dummy expande demasiado el espacio de features y se pueden presentar colinealidades innecesarias. Se consideró ajustar este efecto aplicando una reducción de dimensionalidad con PCA a un punto donde se podia conservar al menos mas del 95% de la varianza total de los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 62828,
     "status": "ok",
     "timestamp": 1602036614074,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "T3b0qvGTdcfc",
    "outputId": "f28a9b87-91e8-40c3-d0aa-cce85cd97396"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=400, svd_solver='full')\n",
    "pca.fit(X)\n",
    "print('explained variance: ', np.sum(pca.explained_variance_ratio_))\n",
    "Xpca = pca.transform(X)\n",
    "y = y = ml.item_df['sold_quantity'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xpca, y, random_state=44, test_size = 0.1)\n",
    "print('array shape: ', Xpca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O10-_C6DGqhe"
   },
   "source": [
    "Aplicando a estos nuevos features los mismos dos modelos del paso anterior, se logra mejores ajustes pero eso no elimina el overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "executionInfo": {
     "elapsed": 127247,
     "status": "ok",
     "timestamp": 1602036679813,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "-1-ZfpJizK5H",
    "outputId": "bef442d4-f255-45ac-82dc-88f25f87ade9"
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_depth=40, random_state=0)\n",
    "plot_model_performance(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "executionInfo": {
     "elapsed": 144940,
     "status": "ok",
     "timestamp": 1602036700258,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "ca390liNyeLa",
    "outputId": "352cedf6-bba5-48c9-ce8b-1f0091a6dcbb"
   },
   "outputs": [],
   "source": [
    "model = MLPRegressor(random_state=1, max_iter=10000)\n",
    "plot_model_performance(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oN5miJDi8fIv"
   },
   "source": [
    "## DNN regressor over PCA predictors\n",
    "\n",
    "Luego de los experimentos anteriores los cuales se escogieron porque fueron los unicos modelos que resultaban en un R2 score decente para la data de training, se considera un modelo de DNN con tensorflow, para explorar modelos con mayor versatilidad y donde posiblemente ciertos parametros permitan controlar mejor el overfit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1602039874495,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "mkBCaek_lTdu"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  tf.compat.v1.reset_default_graph()\n",
    "  model = keras.Sequential([ \n",
    "            layers.Dense(300, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "            layers.Dropout(0.5), \n",
    "            layers.Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1)\n",
    "  ])\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model\n",
    "\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  #plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  #plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGfF4HHiHTbv"
   },
   "source": [
    "Con la arquitectura de NN considerada anteriormente usando la API sequential de Keras, se entrena un modelo por 2000 epochs. Dentro de la arquitectura se consideran las siguientes reglas para atenuar overfit: \n",
    "\n",
    "* uso de Dropout entre capas\n",
    "* uso de regularizadores en las capas\n",
    "\n",
    "estos tips se usan por recomendacion de la documentacion de Tensorflow. \n",
    "https://www.tensorflow.org/tutorials/keras/overfit_and_underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "executionInfo": {
     "elapsed": 167074,
     "status": "ok",
     "timestamp": 1602041717134,
     "user": {
      "displayName": "francisco javier Camacho rodríguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gih_Sn4zDsTdgE6io5q-HdmNbhLPaCnF29qVMXA9g=s64",
      "userId": "17877708759957967531"
     },
     "user_tz": 300
    },
    "id": "HqvraGd36NCQ",
    "outputId": "48507634-6503-4b43-bd5f-87206483bd46"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "history = model.fit(X_train, y_train, epochs=2000, verbose = 0)\n",
    "plot_loss(history)\n",
    "predictions_train = model.predict(X_train)\n",
    "sns.regplot(x = y_train, y = predictions_train, \n",
    "            label='train R2 = {}'.format(round(r2_score(y_train, predictions_train), 3)))\n",
    "predictions_test = model.predict(X_test)\n",
    "sns.regplot(x = y_test, y = predictions_test, \n",
    "            label='test R2 = {}'.format(round(r2_score(y_test, predictions_test), 3)))\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL0A9igkCeoP"
   },
   "source": [
    "# Conclusiones \n",
    "\n",
    "A pesar de los esfuerzos por reducir el overfit en el modelo de DNN, el fenomeno no desaparece. Se presume que esto sucede principalmente por tres razones: \n",
    "\n",
    "1. `Tamaño del dataset`: el dataset es muy pequeño, tal vez con mayores datasets se pueda evidenciar una relación más clara entre predictores y variable objetivo. \n",
    "2.  `Gaps de sparsity`: el último grafico en escala logarítmica evidencia que los datos en su mayoria estan concentrados en regiones muy densas dejando amplios vacios y por otro lado hay otros datapoints que estan muy alejados. Parece que el comportamiento es muy herratico para publicaciones con bajos numeros de `sold_quantity`, entonces es probable que en el espacio original no exista una relacion sencilla o incluso uno-a-uno entre las variables o que esto haga muy dificil para el modelo encontrar la superficie que mejor estima los datos.\n",
    "3. `las variables de antiguedad y precio de la publicación no fueron consideradas`: como se habia pensado inicialmente, variables como la antiguedad de una publicacion y el precio del item correspondiente hubieran sido buenos predictores, por si solos eso no se evidencia, pero tal vez si se combinan con las variables dummy, se pueda tener una mejor prediccion, ya que debe ser logico que estas variables tengan influencia en el `sold_quantity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEwZDdz6Cn8L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPpnWI9TdvhI1x75tJT0B/C",
   "collapsed_sections": [],
   "mount_file_id": "1WLUiGPp-SVHSmyvuE-Bj0t8KC-PXhJkX",
   "name": "ModeloPredictivo_MercadoLibre.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
